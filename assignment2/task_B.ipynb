{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "## Understanding the problem\n",
    "### Description\n",
    "**Cliff Walking** is a *single-agent*, *deterministic*, and *discrete* reinforcement learning problem.\n",
    "- Single agent means that there is only one agent. *aka not multi-agent*.\n",
    "- Deterministic means that given an action-state pair `(s, a)`, the transition function will always yield state `s'`. *aka not stochastic*.\n",
    "- Discrete means that there is a countable amount of actions and observations. *aka not continuous*.\n",
    "\n",
    "The *goal of the agent* is to go from the starting position to the target position without falling off the cliff.\n",
    "\n",
    "The *action space* is of shape (4,). The agent is equipped with 4 directional movements to navigate the environment.\n",
    "\n",
    "The *observation space* is of shape (48,). An observation represents the position of the agent on the map. There are 37 allowed states, and ` states the agent must avoid.\n",
    "\n",
    "The *reward* is: -1 for each step the agent takes, and -100 for falling off the cliff. No reward is necessary for reaching the goal as the target position represents the *terminal state*, meaning that for the agent to maximize its reward, it is implied that it must reach the target position.\n",
    "\n",
    "### Optimal policy\n",
    "The optimal policy to solve this problem would be to move right alongside the cliff and reach the finish point without going north, minimizing the travel cost.\n",
    "\n",
    "### Highest/Lowest reward achievable\n",
    "The **highest** reward achievable is -13, as the agent needs a minimum of 13 steps to reach the terminal state.\n",
    "\n",
    "*For the pre-configured parameters of this assignment*, the **lowest** reward achievable is -1099, as the agent can travel for 999 steps, and on their final 1000th step they can walk off the cliff for -100 reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline strategy: Equiprobable Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_MODE = \"graphic\"  # choose between \"graphic\" or text;  graphic mode needs the pygame package to be installed\n",
    "RENDER_FREQUENCY = 0.2  # output the game state at most every X seconds\n",
    "\n",
    "env = gym.make('CliffWalking-v0', render_mode = \"rgb_array\" if RENDER_MODE == \"graphic\" else \"ansi\") # initialize the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dumb agent that applies a policy that makes it run in circles\n",
    "#   you can use this example agent class as a temple for implementing smarter ones\n",
    "# Feel free to add methods if you consider it necessary\n",
    "class DumbAgent():\n",
    "    def __init__(self):\n",
    "        self.previous_action = -1 # This agent needs to remember which direction he was going before so he can make nice circles\n",
    "\n",
    "    # decide what action to take in the provided state by applying a certain policy\n",
    "    def select_action(self, state):\n",
    "        new_action = (self.previous_action + 1) % 4 # this agents policy is to run in circles\n",
    "        self.previous_action = new_action\n",
    "        return new_action\n",
    "    \n",
    "    # do the learning (e.g. update the Q-values, or collect data to do off-policy learning)\n",
    "    def update(self, old_state, action, reward, new_state):\n",
    "        pass # this agent just does not learn anything \\_o_/\n",
    "\n",
    "    # reset the agent to its initial state after an epoch. Can also be used to perform learning after an epoch)\n",
    "    def reset(self, state):\n",
    "        self.previous_action = -1\n",
    "\n",
    "def to_coord(state_id):\n",
    "    # we return the (x, y) coorinates, in the description page the use [y, x] to describe the locations\n",
    "    return (state_id % 12, state_id // 12)\n",
    "\n",
    "actions = {\n",
    "    0: \"up\",\n",
    "    1: \"right\",\n",
    "    2: \"down\",\n",
    "    3: \"left\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_game(env, wait_time_s = RENDER_FREQUENCY):\n",
    "    if RENDER_MODE == \"graphic\":\n",
    "        img = env.render()  # show the current state of the game (the environment)\n",
    "        plt.ion()\n",
    "        if display_game.plt_im is None:\n",
    "            display_game.plt_im = plt.imshow(img)\n",
    "        else:\n",
    "            display_game.plt_im.set_data(img)\n",
    "        plt.draw()\n",
    "        plt.show()\n",
    "        plt.pause(0.001)\n",
    "    else:\n",
    "        img_ansi = env.render()  # show the current state of the game (the environment)\n",
    "        print(img_ansi)\n",
    "    time.sleep(wait_time_s)\n",
    "display_game.plt_im = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns the number of steps taken and the ending reason (-1 if fallen off, 0 if survived but out of steps, 1 if reached goal)\n",
    "def run_episode(agent, max_steps = 1000, muted = False):\n",
    "    observation, info = env.reset() # restart the game\n",
    "    agent.reset(observation) # reset the agent to its initial state\n",
    "    if not muted:\n",
    "        print(f\"Starting in position: {to_coord(observation)}\")\n",
    "\n",
    "    for k in range(max_steps):\n",
    "        # HINT: you may want to disable displaying of the game to run the experiments\n",
    "        if not muted:\n",
    "            display_game(env)\n",
    "\n",
    "        action = agent.select_action(observation)  # select an action based on the current state\n",
    "        new_observation, reward, terminated, truncated, info = env.step(action)  # perform the action and observe what happens \n",
    "        fallen_off_cliff = (reward == -100)  # Beware! we cannot check for cliff state because the environment automatically returns us to the starting position when falling off a cliff\n",
    "        goal_reached = terminated  # if we reach the goal state, the environment returns terminated = True \n",
    "\n",
    "        agent.update(observation, action, reward, new_observation)  # perform some learning if the agent is capable of it\n",
    "\n",
    "        if not muted:\n",
    "            print(f\"Action determined by agent: {actions[action]}\")\n",
    "            print(f\"Reward for action: {actions[action]} in state: {observation} is: {reward}\")\n",
    "            print(f\"New state is: {new_observation}\")\n",
    "\n",
    "        if goal_reached:\n",
    "            if not muted:\n",
    "                print(f\"Goal reached after terminated after {k+1} steps\\n\\n\")\n",
    "            return k+1, 1  # we reached the goal\n",
    "        elif fallen_off_cliff:\n",
    "            if not muted:\n",
    "                print(f\"Fell off the cliff after {k+1} steps\\n\\n\")\n",
    "            \n",
    "            return k+1, -1  # we fell off the cliff\n",
    "\n",
    "        observation = new_observation\n",
    "\n",
    "    if not muted:\n",
    "        print(f\"Survived for {k+1} steps but goal not reached\\n\\n\")\n",
    "    return k+1, 0  # we survived but did not reach the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(agent, episodes = 500):\n",
    "    win_count = 0\n",
    "    mute_output = False # you may want to mute the output if you run a lot of episodes\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        steps, reason = run_episode(agent, muted=mute_output)\n",
    "        if reason == 1:\n",
    "            win_count += 1\n",
    "    print(f\"Reached goal {win_count} times out of {episodes} games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_agent = DumbAgent()\n",
    "run_experiment(dumb_agent)\n",
    "env.close() # end the game"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
