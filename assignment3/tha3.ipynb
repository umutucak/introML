{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import ndarray, float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Activation and cost functions.\"\"\"\n",
    "    \n",
    "class MSE():\n",
    "    \"\"\"Mean Squared Error.\"\"\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"mse\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def __call__(e):\n",
    "        \"\"\"Mean Squared Error.\n",
    "        \n",
    "        `1/2 * sum(e^2)`.\n",
    "        \"\"\"\n",
    "        return np.divide(np.sum(e**2), 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def diff(e):\n",
    "        \"\"\"Derivative of Mean Squared Error.\n",
    "        \n",
    "        `e`.\n",
    "        \"\"\"\n",
    "        return e\n",
    "\n",
    "class Sigmoid():\n",
    "    \"\"\"Sigmoid.\"\"\"\n",
    "    def __str__(self) -> str:\n",
    "        return \"sigmoid\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def __call__(x):\n",
    "        \"\"\"Sigmoid activation function.\n",
    "\n",
    "        `1 / (1 + e^-x)`\n",
    "        \n",
    "        Args:\n",
    "            x `<ndarray[float32]`: `wx + b` vectorized for our use case\n",
    "        \n",
    "        Returns:\n",
    "            `sig(x)`\n",
    "        \"\"\"\n",
    "\n",
    "        return np.divide(1, np.add(1, np.exp(-x)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def diff(x):\n",
    "        \"\"\"Derivative of the Sigmoid activation function.\n",
    "\n",
    "        `e^-x / (e^-x + 1)^2`\n",
    "        \n",
    "        Args:\n",
    "            x `<ndarray[float32]`: `wx + b` vectorized for our use case\n",
    "        \n",
    "        Returns:\n",
    "            `sig'(x)`\n",
    "        \"\"\"\n",
    "        return np.divide(np.exp(-x), np.power(np.add(np.exp(-x), 1), 2))\n",
    "\n",
    "class ReLU():\n",
    "    \"\"\"ReLU.\"\"\"\n",
    "    def __str__(self) -> str:\n",
    "        return \"relu\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def __call__(x):\n",
    "        return x if x>0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def diff(x):\n",
    "        return 1 if x>0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Network classes.\"\"\"\n",
    "\n",
    "class Layer():\n",
    "    \"Class representing the weights and biases of a layer of the MLP using numpy\"\n",
    "    def __init__(self, shape:tuple[int], activation) -> None:\n",
    "        \"\"\"MLP Layer contructor\n",
    "        \n",
    "        Args:\n",
    "            shape `<tuple[int]>`: shape of the weights/biases\n",
    "        \"\"\"\n",
    "        self.shape = shape\n",
    "        self.activation = activation\n",
    "        self.weights:ndarray[float32] = np.random.rand(shape[0], shape[1])\n",
    "        self.biases:ndarray[float32] = np.random.rand(1, shape[0])\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.shape) + str(self.activation)\n",
    "\n",
    "    def compute(self, x:ndarray[float32]) -> ndarray[float32]:\n",
    "        \"\"\"Calculating `wx + b`\n",
    "        \n",
    "        Args:\n",
    "            x `<ndarray[float32]>`: input data\n",
    "        \"\"\"\n",
    "\n",
    "        return np.add(np.matmul(x, np.transpose(self.weights)), self.biases)\n",
    "\n",
    "class MLP():\n",
    "    \"\"\"Class representing an MLP with the preconfiguration from the assignment.\"\"\"\n",
    "    def __init__(self, layers:ndarray[tuple[float32, any]]) -> None:\n",
    "        \"\"\"Building an MLP with the given layer/neuron config.\n",
    "        \n",
    "        Args:\n",
    "        - layers `<ndarray[tuple[float32]]>`: array representing the neurons of each layer.\"\"\"\n",
    "        \n",
    "        \n",
    "        self.layers = [Layer((layers[i+1][0], layers[i][0]), layers[i][1]) for i in range(len(layers)-1)]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str([str(l) for l in self.layers])\n",
    "\n",
    "    def feed_forward(self, x:ndarray[float32]):\n",
    "        \"\"\"One cycle of feed forward.\n",
    "        \n",
    "        Args:\n",
    "            x `<ndarray[float32]>: One batch of training data.\n",
    "            \n",
    "        Returns:\n",
    "            y_pred `<float32>`: Predicted y value.\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "        return \n",
    "\n",
    "    def back_propagation():\n",
    "        pass\n",
    "\n",
    "layers = np.array([(2, Sigmoid()), (10, Sigmoid()), (10, Sigmoid()), (2, Sigmoid())])\n",
    "mlp = MLP(layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
